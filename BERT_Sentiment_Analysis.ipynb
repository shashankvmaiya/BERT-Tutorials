{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT - Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQUIQGANA96I"
      },
      "source": [
        "# Sentiment Analysis with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hivbLURpB3b2"
      },
      "source": [
        "In this notebook we will train BERT to perform a sentiment analysis task. The same methodology extends to any kind of text classification task. As the original [paper](https://arxiv.org/abs/1810.04805) suggests, we add a single feed foward layer on top of BERT and fine-tune it for our task. Prior to going through this notebook, please make sure you go through and understand the \"BERT - The Basics\" notebook. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um30Suwd6uGY"
      },
      "source": [
        "First we need to install and import the necessary packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM4ojFgUWoP5"
      },
      "source": [
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkMB3jbIXNvx"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch \n",
        "from transformers import *\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD2cnYF_xncd"
      },
      "source": [
        "RANDOM_SEED = 0\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIL-BEDMv87q"
      },
      "source": [
        "# Data and Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paeygW7pXr2v"
      },
      "source": [
        "We're going to work with a dataset of fine foods reviews from Amazon. The original dataset spans a period of more than 10 years, including ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. The `Score` column consists of scores ranging from 1 to 5 The original dataset can be found [here](https://www.kaggle.com/snap/amazon-fine-food-reviews)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPUWKKnPZk8J"
      },
      "source": [
        "Run the following cells to load the data into a Pandas Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOjtHbl8YiC6"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzpMMgCqZqhy"
      },
      "source": [
        "data_id = \"1AuYKU1xhGTAUoFbZe3kg3jsKqepfXO_F\"\n",
        "df_downloaded = drive.CreateFile({\"id\": data_id}) \n",
        "df_downloaded.GetContentFile(\"Reviews.csv\")\n",
        "df = pd.read_csv(\"Reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0A9a_IWdm3B"
      },
      "source": [
        "Run the below cell to see what the data looks like. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FpaEbuOdHEZ"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMHuXnT79ULU"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlxcTrfeNkfa"
      },
      "source": [
        "We substract one from each score so that rather than ranging from 1 to 5 they range from 0 to 4. We also keep track of the number of classes for later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB2Yiwe8eZu0"
      },
      "source": [
        "df[\"Score\"] -= 1\n",
        "N_CLASSES = len(df[\"Score\"].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfgtz-zVdveU"
      },
      "source": [
        "In the pre-processing a few things we want to consider. First, we want to make sure that there are reasonbly even number of samples corresponding to each score 0, 1, 2, 3, 4. If this is not the case, it may not be reasonable to expect our model to be able to infer the score on this level of granularity. We can easily plot this below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7qDGRr0s3-J"
      },
      "source": [
        "def plot_scores(df):\n",
        "  score_counts = df[\"Score\"].value_counts().sort_index().values\n",
        "  plt.bar(range(5), score_counts)\n",
        "\n",
        "plot_scores(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjnW83yfqOt"
      },
      "source": [
        "We see there is an overwhelming number of reviews with score five. This type of imbalance can make it harder for the model to learn to distinguish between the five classes as it may rely on exploiting the prior probabilities to optimize it's loss objective. Thankfully, in our case we have a very large amount of data. Many examples of fine-tuning BERT happen on 1,000s or 10,000s of samples. Due to our abundance of data we can afford to throw out some of our data (in fact, we will need to in order to make training take a reasonable amount of time). We will do so in a way that evens out the number of samples per class. Precisely, we will pick a number $n$ and randomly select $n$ samples from each class.\n",
        "\n",
        "Note this is not the neccesarily the most efficient or best way of dealing with class imbalance:\n",
        "\n",
        "1. We don't need to perfectly balance the data. In fact, doing so means the data we're training on IS NOT representative of the actual sample. Depending on our goal, it may be important for the model to learn about the prior probability of each sample. \n",
        "\n",
        "2. This data could be kept for use in our test or validation set.\n",
        "\n",
        "3. For all we know, this imbalance potentially won't result in worse training/valdiation performance. We'd have to experiment to try it out.\n",
        "\n",
        "If you're curious about these sorts of questions, we encourage you to tinker with this part of the pre-processing stage on your own time (more on this at the end of the notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSaoV-u2qOwP"
      },
      "source": [
        "\n",
        "# Randomly select min_count number of samples for each score\n",
        "def randomly_balance(df, min_count=0):\n",
        "  score_counts = df[\"Score\"].value_counts().sort_index().values\n",
        "  if min_count <= 0:\n",
        "    min_count = min(score_counts)\n",
        "  chosen_indices = np.zeros(len(score_counts)*min_count)\n",
        "  for idx, score in enumerate(df[\"Score\"].unique()):\n",
        "    chosen_indices[idx*min_count : (idx +1)*(min_count)] = np.random.choice(df[df[\"Score\"] == score].index, min_count)\n",
        "  df_balanced = df.loc[chosen_indices]\n",
        "  return df_balanced\n",
        "\n",
        "df_balanced = randomly_balance(df, 1000)\n",
        "plot_scores(df_balanced)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_04XmgguvtqU"
      },
      "source": [
        "We'll go ahead and split our data into training, validation, and test sets. Additionally we'll discard unnecessary columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjs5ePQ-4CaM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_not_train = train_test_split(df_balanced,\n",
        "                                          test_size=0.2,\n",
        "                                          random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_not_train,\n",
        "                                   test_size=0.5,\n",
        "                                   random_state=RANDOM_SEED)\n",
        "\n",
        "# Get rid of unnecessary columns\n",
        "df_train = df_train[[\"Text\", \"Score\"]]\n",
        "df_val = df_val[[\"Text\", \"Score\"]]\n",
        "df_test = df_test[[\"Text\", \"Score\"]]\n",
        "\n",
        "print(\"Number of training samples: \", len(df_train))\n",
        "print(\"Number of validation samples: \", len(df_val))\n",
        "print(\"Number of test samples: \", len(df_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9ttOvi5vsJd"
      },
      "source": [
        "Lastly, BERTbase (the default model we'll try out) can only take in sequences with up to 512 tokens. For sequences longer than 512 we'll be forced to reduce the sequence length with some workaround, for example truncation. We want to make sure that for the most part, BERTbase will be able to input our reviews without truncating too much information. To check, we make a histogram the numer of tokens of reviews in our training data set.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_yw_leggBT"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlV6KsAhiSfa"
      },
      "source": [
        "tokenized_text_dict = tokenizer.batch_encode_plus(df_train[\"Text\"].tolist(),\n",
        "                                                  max_length=None, \n",
        "                                                  pad_to_max_length=False,\n",
        "                                                  return_token_type_ids=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z0zQSC2PNWY"
      },
      "source": [
        "sample_token_lengths = [len(tokens) for tokens in tokenized_text_dict['input_ids']]\n",
        "print(\"Percentage of samples with >512 tokens: \", sum([1 if x > 512 else 0 for x in sample_token_lengths])/len(sample_token_lengths))\n",
        "plt.hist(sample_token_lengths, bins=100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGLOMLNi8eQZ"
      },
      "source": [
        "We see that the vast majority of samples have $<=$ 512 tokens, so we can proceed as planned. As an added comfort, we can be reasonably confident that, for even the longer reviews, the \"sentiment\" of the review will be captured in the first 512 tokens. With this in mind, we could even consider using a smaller `MAX_LENGTH`. It's best practice to use as small a `MAX_LENGTH` as possible without damaging model performance for efficiency/compute reasons.\n",
        "\n",
        "If you're curious what to do when exceeding BERT's 512 limit, check out [this](https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification) StackOverflow post or this [blog's](https://medium.com/dataseries/why-does-xlnet-outperform-bert-da98a8503d5b) discussion of how XLNnet handles this issue. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCIn_M2x9Xwl"
      },
      "source": [
        "### Making a Torch DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNTSuKVP-yif"
      },
      "source": [
        "Now that we're going to be working with PyTorch objects, we create a device variable which will be used to represent which device a torch tensor is on (`cpu` or `cuda`). We are running this notebook on a GPU, so you should expect the output to be `cuda`. If it is not, go to Runtime -> Change runtime type and select GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8VTMbep_ZhL"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjudxRRQ9jrS"
      },
      "source": [
        "We're going to construct a PyTorch Dataset and DataLoader. The DataLoader will help us load in minibatches when we train and evaluate the model. Read more about these objects [here](https://pytorch.org/docs/stable/data.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiAH8foW9yKO"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AmazonReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, labels, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    label = self.labels[item]\n",
        "    encoding = self.tokenizer.encode_plus(review,\n",
        "                                          add_special_tokens=True,\n",
        "                                          max_length=self.max_len,\n",
        "                                          return_token_type_ids=False,\n",
        "                                          pad_to_max_length=True,\n",
        "                                          return_attention_mask=True,\n",
        "                                          return_tensors='pt')\n",
        "    item_dict = {'review_text': review,\n",
        "                 'input_ids': encoding['input_ids'].flatten(),\n",
        "                 'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                 'labels': torch.tensor(label, dtype=torch.long)}\n",
        "    return item_dict\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = AmazonReviewDataset(reviews=df[\"Text\"].to_numpy(),\n",
        "                           labels=df[\"Score\"].to_numpy(),\n",
        "                           tokenizer=tokenizer,\n",
        "                           max_len=max_len)\n",
        "  dl = DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
        "  return dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKd6G6SGpEDz"
      },
      "source": [
        "We construct a DataLoader for the training, validation, and test set. Due to RAM constraints we can only afford to take batches of size 8 (any more will crash the notebook) when fine-tuning BERT. This means for large datasets, training for many epochs can take quite a long time! If we freeze the weights of BERT we can work with a larger batch size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_TiOMBdeiNW"
      },
      "source": [
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "N_TRAINING_SAMPLES = len(df_train)\n",
        "N_VAL_SAMPLES = len(df_val)\n",
        "N_TEST_SAMPLES = len(df_test)\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTjFomu_Aag4"
      },
      "source": [
        "We can take a peek at an example batch from our training data loader to make sure everything is in order. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGRQnEEeA1ur"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "print(data.keys())\n",
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['labels'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug-_fUUPD9kx"
      },
      "source": [
        "# Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2I9M2FLEDCX"
      },
      "source": [
        "We're going to use the built in `BertForSequenceClassfication` model. The model applies dropout (as regulaization) to the final hidden state of the [CLS] token and inputs it to a linear layer which outputs logits for possible classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlbNXj8dFeAX"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odVmqo4oty_f"
      },
      "source": [
        "We define two helper functions that will train/evaluate the model over one epoch (one loop through the dataset that is passed in). Along the way we keep track of useful statistics which help us monitor our model during training and study the training process afterwards. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfbmefmWHxuc"
      },
      "source": [
        "from torch import nn, optim\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Trains the model over one epoch\n",
        "\n",
        "def train_epoch(model, data_loader, device, optimizer, scheduler):\n",
        "  \n",
        "  # Put model in training mode\n",
        "  model = model.train()\n",
        "  \n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  num_samples = 0\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  for d in tqdm.notebook.tqdm(data_loader):\n",
        "\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    labels = d[\"labels\"].to(device)\n",
        "    num_samples += len(input_ids)\n",
        "\n",
        "    # Forward pass. When we feed in labels, output[0] is the loss and output[1] is the logits\n",
        "    output = model(input_ids=input_ids,\n",
        "                   attention_mask=attention_mask,\n",
        "                   labels=labels)\n",
        "    \n",
        "    _, preds = torch.max(output[1], dim=1)\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "   \n",
        "    all_preds.extend(preds.tolist())\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "    loss = output[0]\n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    # Take gradient step\n",
        "    loss.backward()\n",
        "    MAX_NORM = 1\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_NORM)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "  acc =  correct_predictions.double() / num_samples\n",
        "  mean_loss = np.mean(losses)\n",
        "  conf_mat = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "  return acc, mean_loss, conf_mat\n",
        "\n",
        "def eval_epoch(model, data_loader, device):\n",
        "  \n",
        "  # Put model in eval mode\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  num_samples = 0\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in tqdm.notebook.tqdm(data_loader):\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      labels = d[\"labels\"].to(device)\n",
        "      num_samples += len(input_ids)\n",
        "\n",
        "      # Forward pass. When we feed in labels, output[0] is the loss and output[1] is the logits\n",
        "      output = model(input_ids=input_ids,\n",
        "                     attention_mask=attention_mask,\n",
        "                     labels=labels)\n",
        "    \n",
        "      _, preds = torch.max(output[1], dim=1)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "      loss = output[0]\n",
        "      losses.append(loss.item())\n",
        "   \n",
        "      all_preds.extend(preds.tolist())\n",
        "      all_labels.extend(labels.tolist())\n",
        "\n",
        "  acc =  correct_predictions.double() / num_samples\n",
        "  mean_loss = np.mean(losses)\n",
        "  conf_mat = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "  return acc, mean_loss, conf_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpV6UcQtGAR1"
      },
      "source": [
        "Now we can go ahead and train the model! We set a fixed number of epochs and learning rate (both tunable parameters). Best practice would be to save the model weights that achieve best validation performance while training (we don't do this). With our current configuration this will take ~30m-60m. To decrease training time we can either train on less data or freeze the BERT pre-trained weights and increase our batch size. \n",
        "\n",
        "**If you get a CUDA out of memory error while training restart the notebook (Runtime -> Factory reset runtime) and try again. If that doesn't work decrease batch size.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhfz7S7MF67d"
      },
      "source": [
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# We reseed when making a model so the weights are initiliazed identitcally each time\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, \n",
        "                                                      num_labels=N_CLASSES)\n",
        "model = model.to(device) # moves model to GPU is GPU is avaliable\n",
        "\n",
        "# We change the dropout probability from the default config\n",
        "\n",
        "DROPOUT_PROB = 0.5\n",
        "model.dropout = nn.Dropout(DROPOUT_PROB)\n",
        "\n",
        "# Uncomment this to freeze the pre-trained BERT weights\n",
        "#for param in model.bert.parameters():\n",
        "#    param.requires_grad = False\n",
        "\n",
        "# Passing only those parameters that explicitly require grad in case we freeze BERT weights\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
        "\n",
        "#optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * NUM_EPOCHS\n",
        "\n",
        "# This function comes from the transformers library\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=total_steps) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Il2ZFZIsIt"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Training time!\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  \n",
        "  train_acc, train_loss, train_conf = train_epoch(model,\n",
        "                                                  train_data_loader,\n",
        "                                                  device,\n",
        "                                                  optimizer,\n",
        "                                                  scheduler)\n",
        "  print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
        "\n",
        "  val_acc, val_loss, val_conf = eval_epoch(model,\n",
        "                                           val_data_loader,\n",
        "                                           device)\n",
        "  print(f'Val loss: {val_loss}, Val accuracy {val_acc}')\n",
        "  \n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  history['train_conf'].append(train_conf)\n",
        "  history['val_conf'].append(val_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCiW_4TTMbqw"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i282dn1PK7lw"
      },
      "source": [
        "We can use our ``history`` dict to study the results of our training. First we'll plot our accuracy and loss over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-mm693pLBQR"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def plot_training_stats(train_list, val_list, title):\n",
        "  plt.plot(train_list, label='train')\n",
        "  plt.plot(val_list, label='val')\n",
        "  plt.title(title)\n",
        "  plt.ylabel(title)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "plot_training_stats(history['train_loss'],\n",
        "                    history['val_loss'],\n",
        "                    'Loss')\n",
        "plot_training_stats(history['train_acc'],\n",
        "                    history['val_acc'],\n",
        "                    'Accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhvZGJfJz0Vq"
      },
      "source": [
        "Accuracy might not be the best way to gauge how well our model is generalizing. Not all mistakes are necessarily as bad as others. For example, if we classify a 1 as a 2, that's not as inexcusable as classifying a 1 as a 5. We present two alterantive metrics for gauging model performance and plot them:\n",
        "\n",
        "1. Off by one accuracy: So long as our predicted label is within one of the true label we'll consider the prediction correct. With constant prior class probabilities, we'd expect a model that randomly assigns labels to get this right ~52% of the time.\n",
        "\n",
        "2. Extreme accuracy: We only consider the classification of 1s, 2s, 4s, and 5s. So long as we don't classify a 1 or 2 as a 4 or 5 or vice versa, we consider the classification correct. A model that ranodmly assigns labels would get this right 50% of the time. \n",
        "\n",
        "We also print out the last confusion matrix on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UhBmGhYzutP"
      },
      "source": [
        "from scipy import linalg\n",
        "\n",
        "def compute_off_acc(conf_mat):\n",
        "  n = len(conf_mat)\n",
        "  off_diag = np.zeros(n)\n",
        "  off_diag[0] = 1\n",
        "  off_diag[1] = 1\n",
        "  M = linalg.toeplitz(off_diag, off_diag)\n",
        "  return np.sum(conf_mat * M)/np.sum(conf_mat)\n",
        "\n",
        "# hard-coded for 5 classes\n",
        "\n",
        "def compute_ext_acc(conf_mat):\n",
        "  conf_mat = np.delete(conf_mat, 2, 0)\n",
        "  conf_mat = np.delete(conf_mat, 2, 1)\n",
        "  return (np.sum(conf_mat[0:2, 0:2]) + np.sum(conf_mat[2:4, 2:4]))/np.sum(conf_mat)\n",
        "\n",
        "\n",
        "off_train_acc = []\n",
        "off_val_acc = []\n",
        "ext_train_acc = []\n",
        "ext_val_acc = []\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "  train_conf = history['train_conf'][i]\n",
        "  val_conf = history['val_conf'][i]\n",
        "  off_train_acc.append(compute_off_acc(train_conf))\n",
        "  off_val_acc.append(compute_off_acc(val_conf))\n",
        "  ext_train_acc.append(compute_ext_acc(train_conf))\n",
        "  ext_val_acc.append(compute_ext_acc(val_conf))\n",
        "\n",
        "plot_training_stats(off_train_acc,\n",
        "                    off_val_acc,\n",
        "                    'Off by One Accuracy')\n",
        "plot_training_stats(ext_train_acc,\n",
        "                    ext_val_acc,\n",
        "                    'Extreme Accuracy')\n",
        "\n",
        "print(history['val_conf'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Ui3VGOW17K"
      },
      "source": [
        "For the sake of comparison we compare this to the performance of our un-trained model. Our model is clearly learning quite a bit! And it's doing so after training on only 1-2 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91mpybkzVut4"
      },
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "random_model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, \n",
        "                                                             num_labels=N_CLASSES)\n",
        "random_model = random_model.to(device)\n",
        "r_val_acc, r_val_loss, r_val_conf = eval_epoch(random_model,\n",
        "                                               val_data_loader,\n",
        "                                               device)\n",
        "print(\"Un-trained acc.: \", r_val_acc.item())\n",
        "print(\"Un-trained off by one acc.: \",  compute_off_acc(r_val_conf))\n",
        "print(\"Un-trained ext one acc.: \",  compute_ext_acc(r_val_conf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhYmB9k1FYQs"
      },
      "source": [
        "#### <font color='red'>Experiment yourself!</font>\n",
        "\n",
        "Although we've fully walked through training a BERT classification model, there's still a number of things to be done. Our model has the ability to fit data extremely well and performs well by some metrics, but it's clearly overfitting the training data and there's quite a lot of room for improvement. Ideally we would train on a lot more of our data with larger batch size, but we're limited by Colab's resources. We suggest a number of potential avenues of feasible further experimentation/exploration below. The 5th is the most time consuming, but will give you the best understanding of the material. \n",
        "\n",
        "*Note: Prior to experimenting one should hold out a test data set. Since some of these experiments involve altering how the data is processed you should be extra careful about this.*\n",
        "\n",
        "1. Find the examples where the model is misclassifying and print them out. Can you figure out why the model is making mistakes? Are the mistakes reasonable?\n",
        "2. In pre-processing, don't balance the classes. See how this impacts model performance and the confusion matrices during training. \n",
        "3. There are a number of hyper-parameters to tune (e.g. amount of training data, number of epochs, batch size, max length, dropout, learning rate). Experiment with how changing these effects your results. A reasonble place to start is adding a lot more training data and training for a very small number of epochs.\n",
        "4. To a human, the difference between a 1 and 2 star or 4 and 5 star review may be immaterial. We chose to train the model on all five classes to give an example of multi-class classifcation. To predict to this level of granularity is a hard, noisy task. As we hinted when we came up with different metrics for evaluating model performance, it isn't necessarily the most reasonable thing to do. Try labelling reviews as negative (1 and 2 star) and positive (4 and 5 star) and re-train the model on newly labelled data. See how the model performs on this binary classification task. \n",
        "5. Instead of using `BertForSequenceClassification` make your own model! Here's some motivation: Our model is likely overfitting because we're training such a large number of parameters on a small dataset. One solution is to keep the same model but freeze BERT's pre-trained weights. This greatly reduces the number of trainable parameters and also allows us to significantly increase batch size without crashing the notebook, meaning we can train on a lot more data. If you try this out and you'll see pretty poor results. This is likely because, without fine tuning, the final hidden state of the [CLS] token isn't neccesarily a good representation of the sentence. Try defining your own model class which inputs a different sentence representation into the final linear layer rather than the outputted [CLS] hidden state. Then try training your model with BERT's pretrained weights frozen. Make sure your model class' `forward()` function's output is compatible with `train_epoch()` and `eval_epoch()`. As an example of an alternate sentence representation, you can average the final hidden states of all the tokens in the sequence.\n",
        "\n"
      ]
    }
  ]
}